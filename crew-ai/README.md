# crew-ai

Run ollama locally:
```bash
ollama serve
```
Run a model
```bash
ollama run phi3:mini
```
